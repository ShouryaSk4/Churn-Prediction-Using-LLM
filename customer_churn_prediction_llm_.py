# -*- coding: utf-8 -*-
"""Customer Churn Prediction - LLM .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14re725NMICQWDf-adbHGwGjXxlKarQFz
"""







!pip -q install langchain openai tiktoken chromadb pypdf sentence_transformers InstructorEmbedding faiss-cpu

!pip install kaleido

!pip install python-multipart

!pip install cohere

!pip -q install langchain huggingface_hub transformers sentence_transformers

import os


os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader

# InstructorEmbedding
from InstructorEmbedding import INSTRUCTOR
from langchain.embeddings import HuggingFaceInstructEmbeddings



"""### Load Multiple files from Directory"""

# connect your Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive"

from langchain.document_loaders import TextLoader

loader = DirectoryLoader(f'{root_dir}/Documents/', glob="./*.txt", loader_cls=TextLoader)
documents = loader.load()



"""### Divide and Conquer"""

text_splitter = RecursiveCharacterTextSplitter(
                                               chunk_size=250,
                                               chunk_overlap=20)

texts = text_splitter.split_documents(documents)

texts[0]

len(texts)

"""### Get Embeddings for OUR Documents"""

!pip install faiss-cpu

import pickle
import faiss
from langchain.vectorstores import FAISS

def store_embeddings(docs, embeddings, sotre_name, path):

    vectorStore = FAISS.from_documents(docs, embeddings)

    with open(f"{path}/faiss_{sotre_name}.pkl", "wb") as f:
        pickle.dump(vectorStore, f)

def load_embeddings(sotre_name, path):
    with open(f"{path}/faiss_{sotre_name}.pkl", "rb") as f:
        VectorStore = pickle.load(f)
    return VectorStore





"""### HF Instructor Embeddings"""

from langchain.embeddings import HuggingFaceInstructEmbeddings

instructor_embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl",
                                                      model_kwargs={"device": "cuda"})

Embedding_store_path = f"{root_dir}/Embedding_store"

store_embeddings(texts,
                  instructor_embeddings,
                  sotre_name='instructEmbeddings',
                 path=Embedding_store_path)

db_instructEmbedd = load_embeddings(sotre_name='instructEmbeddings',
                                    path=Embedding_store_path)

#db_instructEmbedd = FAISS.from_documents(texts, instructor_embeddings)

retriever = db_instructEmbedd.as_retriever(search_kwargs={"k": 3})

retriever.search_type

retriever.search_kwargs

docs = retriever.get_relevant_documents("gender Male SeniorCitizen 0 Partner No Dependents No PhoneService Yes MultipleLines No InternetService DSL OnlineSecurity Yes OnlineBackup Yes DeviceProtection No TechSupport No StreamingTV No StreamingMovies No Contract Month-to-month PaperlessBilling Yes PaymentMethod Mailed check MonthlyCharges 53.85 TotalCharges 108.15")

docs[0]

from langchain import PromptTemplate
custom_prompt_template = """you will be given customer data and using that parameters you will hve to pridict if the cutomer will churn or not

Context: {context}
Question: {question}

Only return the helpful answer below and nothing else.
Helpful answer:
"""

def set_custom_prompt():
    """
    Prompt template for QA retrieval for each vectorstore
    """
    prompt = PromptTemplate(template=custom_prompt_template,
                            input_variables=['context', 'question'])
    return prompt

prompt = set_custom_prompt()



!pip install ctransformers
from langchain.llms import CTransformers


    # Load the locally downloaded model here
llm = CTransformers(
    model = "TheBloke/Llama-2-7B-Chat-GGML",
    model_type="llama",
    max_new_tokens = 128,
)



qa_chain_instrucEmbed= RetrievalQA.from_chain_type(llm=llm,
                                      chain_type='stuff',
                                      retriever=retriever,
                                      return_source_documents=True,
                                      chain_type_kwargs={'prompt': prompt}
                                       )



"""### Testing both MODELS"""

## Cite sources

import textwrap

def wrap_text_preserve_newlines(text, width=110):
    # Split the input text into lines based on newline characters
    lines = text.split('\n')

    # Wrap each line individually
    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]

    # Join the wrapped lines back together using newline characters
    wrapped_text = '\n'.join(wrapped_lines)

    return wrapped_text

def process_llm_response(llm_response):
    print(wrap_text_preserve_newlines(llm_response['result']))
    print('\nSources:')
    for source in llm_response["source_documents"]:
        print(source.metadata['source'])

query = 'customer - Demographics AgeGender Male Services PhoneService Yes MultipleLines No Payment PaperlessBilling No PaymentMethod Mailed Charges MonthlyCharges 50-100 TotalCharges >=150, will custer churn or not '

print('-------------------Instructor Embeddings------------------\n')
llm_response = qa_chain_instrucEmbed(query)
process_llm_response(llm_response)

import pandas as pd
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load CSV files
response_df = pd.read_csv('response-50.csv')
prompt_df = pd.read_csv('prompt-50.csv')

# Compare 'response' and 'output' columns directly
y_true = response_df['response']
y_pred = prompt_df['output']

# Generate confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred)

# Compute accuracy, precision, and recall scores
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')

# Plot confusion matrix using seaborn and matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=prompt_df['output'].unique(), yticklabels=prompt_df['output'].unique())
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Print scores
print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')